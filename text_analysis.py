# -*- coding: utf-8 -*-
"""Text Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uxrqyPvcMd54DNFTV6RRrRVsWmN2SR1-
"""

import pandas as pd
import pickle
import ast
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
import numpy as np #
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('punkt')

data_path_num = '/content/drive/MyDrive/thesis-repository/data_17_19/numerical data/'
data_path_text = '/content/drive/MyDrive/thesis-repository/data_17_19/text data/'

rev_17 = pd.read_pickle(f'{data_path_num}review_17.pkl')
rev_18 = pd.read_pickle(f'{data_path_num}review_18.pkl')
rev_19 = pd.read_pickle(f'{data_path_num}review_19.pkl')

"""**Data Preperation**"""

rev_17['auth_len'] =  rev_17['Authors'].apply(lambda x:len(x))
rev_18['auth_len'] =  rev_18['Authors'].apply(lambda x:len(x))
rev_19['auth_len'] =  rev_19['Authors'].apply(lambda x:len(x))


rev_17_one = rev_17[rev_17['auth_len']==1].reset_index(drop=True)
rev_18_one = rev_18[rev_18['auth_len']==1].reset_index(drop=True)
rev_19_one = rev_19[rev_19['auth_len']==1].reset_index(drop=True)

# 2017

rev_17_one.loc[0,'academic_age'] = [5]
rev_17_one.loc[6,'academic_age'] = [38]
rev_17_one.loc[9,'academic_age'] = [20]

rev_17_one = rev_17_one.drop(index=rev_17_one.iloc[15].name)

# 2018

rev_18_one.loc[3,'academic_age'] = [14]
rev_18_one.loc[4,'academic_age'] = [11]
rev_18_one.loc[5,'academic_age'] = [3]
rev_18_one.loc[21,'academic_age'] = [-4]
rev_18_one.loc[26,'academic_age'] = [28]

rev_18_one = rev_18_one.drop([rev_18_one.index[1], rev_18_one.index[6],rev_18_one.index[25]])

# 2019

rev_19_one.loc[40,'academic_age'] = [20]

rev_19_one = rev_19_one.drop([rev_19_one.index[24], rev_19_one.index[26],rev_19_one.index[27],rev_19_one.index[30],
                              rev_19_one.index[32],rev_19_one.index[34]])


rev_19_one['Authors'] = rev_19_one['Authors'].apply(lambda x: x[0])
rev_19_one['academic_age'] = rev_19_one['academic_age'].apply(lambda x: x[0])

rev_18_one['Authors'] = rev_18_one['Authors'].apply(lambda x: x[0])
rev_18_one['academic_age'] = rev_18_one['academic_age'].apply(lambda x: x[0])

rev_17_one['Authors'] = rev_17_one['Authors'].apply(lambda x: x[0])
rev_17_one['academic_age'] = rev_17_one['academic_age'].apply(lambda x: x[0])

def get_age_labels(age):
  if age<=3:
    return 'Junior'
  elif age>3 and age<=10: # keep
    return 'Intermediate'
  elif age>10:
    return 'Senior'

rev_19_one['auth_category'] = rev_19_one['academic_age'].apply(lambda x: get_age_labels(x))
rev_18_one['auth_category'] = rev_18_one['academic_age'].apply(lambda x: get_age_labels(x))
rev_17_one['auth_category'] = rev_17_one['academic_age'].apply(lambda x: get_age_labels(x))

import re

lemma = nltk.wordnet.WordNetLemmatizer()

def processing(text):
  text = text.lower() # lowercase
  text = re.sub("^\d+\s|\s\d+\s|\s\d+$", " ", text) # remove punctuations
  tokenizer = RegexpTokenizer(r'\w+')
  token_list = tokenizer.tokenize(text) # word tokenization
  all_stopwords = stopwords.words('english')
  all_stopwords.remove('not')
  filtered_words = [word for word in token_list if word not in all_stopwords] # stopword removal
  lem_list = ' '.join(filtered_words)
  return lem_list

import re

lemma = nltk.wordnet.WordNetLemmatizer()

def processing(text):
  text = text.lower() # lowercase
  text = re.sub("^\d+\s|\s\d+\s|\s\d+$", " ", text) # remove punctuations
  tokenizer = RegexpTokenizer(r'\w+')
  token_list = tokenizer.tokenize(text) # word tokenization
  all_stopwords = stopwords.words('english')
  all_stopwords.remove('not')
  filtered_words = [word for word in token_list if word not in all_stopwords] # stopwrod removal
  lemma_words = [lemma.lemmatize(w) for w in filtered_words] # word lemmatization
  lem_list = ' '.join(lemma_words)
  return lem_list

def get_review(rev_list):   # combine multiple reviews per title
  rev = ' '.join(rev_list)
  return rev

rev_17_one['rev_new'] = rev_17_one.apply(lambda x: get_review(x['review']),axis=1)

rev_18_one['rev_new'] = rev_18_one.apply(lambda x: get_review(x['review']),axis=1)

rev_19_one['rev_new'] = rev_19_one.apply(lambda x: get_review(x['review']),axis=1)

rev_17_one = rev_17_one[['title','decision','rev_new','auth_category']]
rev_18_one = rev_18_one[['title','decision','rev_new','auth_category']]
rev_19_one = rev_19_one[['title','decision','rev_new','auth_category']]

rev_17_one['rev_SW'] =  rev_17_one.rev_new.apply(lambda x:processing(x))
rev_18_one['rev_SW'] =  rev_18_one.rev_new.apply(lambda x:processing(x))
rev_19_one['rev_SW'] =  rev_19_one.rev_new.apply(lambda x:processing(x))

rev_17_one['rev_lemma'] =  rev_17_one.rev_SW.apply(lambda x: lemma.lemmatize(' '.join(x.split())))
rev_18_one['rev_lemma'] =  rev_18_one.rev_SW.apply(lambda x: lemma.lemmatize(' '.join(x.split())))
rev_19_one['rev_lemma'] =  rev_19_one.rev_SW.apply(lambda x: lemma.lemmatize(' '.join(x.split())))

rev_19_one.head(2)

# from nltk.tag import *
# nltk.download('averaged_perceptron_tagger')

!pip install vaderSentiment

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
vader_sentiment = SentimentIntensityAnalyzer()
from sklearn.feature_extraction.text import CountVectorizer
from numpy import array



def get_common_pos_neg(rev):
  pos_list = []
  neg_list = []
  neu_list = []
  words = rev.split()
  for i in range(len(words)):
    score = vader_sentiment.polarity_scores(words[i])
    if score['compound']>=.05:
      if words[i-1] != 'not':
        pos_list.append(words[i])
      else:
        neg_list.append(f'{words[i-1]}-{words[i]}')
    elif score['compound']<=-.01:
      neg_list.append(words[i])
    else:
      neu_list.append(words[i])

  vectorized = CountVectorizer(vocabulary=list(set(neg_list))).fit([rev])  # list strings contains 1 to 2 words
  counts = vectorized.transform([rev])
  counts = array(counts.sum(axis=0))[0]
  wordcount = {list(set(neg_list))[i]: counts[i] for i in range(len(list(set(neg_list))))}
  common_neg = nltk.FreqDist(wordcount).most_common(10)
  vectorized = CountVectorizer(vocabulary=list(set(pos_list))).fit([rev])  # list strings contains 1 to 2 words
  counts = vectorized.transform([rev])
  counts = array(counts.sum(axis=0))[0]
  wordcount = {list(set(pos_list))[i]: counts[i] for i in range(len(list(set(pos_list))))}
  common_pos = nltk.FreqDist(wordcount).most_common(10)
  return common_pos,common_neg

def get_negating_words(x,y):
  negate_list = []
  for w in range(len(x)):
    if x[w] == 'not':
      nbr_word = x[w+1]
      if y[w+1] == 'JJ' or  y[w+1] == 'JJS' or y[w+1] == 'JJR':
        negate_list.append(f'{x[w]}-{nbr_word}')
  pairs = nltk.FreqDist(negate_list).most_common(10)
  return pairs

"""# **Sentiment Score**

**A. Single Author**
"""

senti_17_one = rev_17_one[['title','decision','auth_category','rev_new']].copy()
senti_18_one = rev_18_one[['title','decision','auth_category','rev_new']].copy()
senti_19_one = rev_19_one[['title','decision','auth_category','rev_new']].copy()


senti_19_one.head()

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import re
import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

"""**Sentiment Score by Normalization**"""

lemma = WordNetLemmatizer()
stop_words = stopwords.words('english')
stop_words.remove('not')

def text_prep(x: str) -> list:
     corp = str(x).lower()
     corp = re.sub('[^a-zA-Z]+',' ', corp).strip()
     tokens = word_tokenize(corp)
     words = [t for t in tokens if t not in stop_words]
     lemmatize = [lemma.lemmatize(w) for w in words]
     return lemmatize

preprocess_tag = [text_prep(i) for i in senti_17_one['rev_new']]
senti_17_one["preprocess_txt"] = preprocess_tag

preprocess_tag = [text_prep(i) for i in senti_18_one['rev_new']]
senti_18_one["preprocess_txt"] = preprocess_tag

preprocess_tag = [text_prep(i) for i in senti_19_one['rev_new']]
senti_19_one["preprocess_txt"] = preprocess_tag

# dataset A

words = pd.read_excel(f'{data_path_text}pos_neg_words.xlsx')
pos_words = words['Positive '].tolist()
neg_words = words['Negative'].tolist()

# calculate sentiment scores from dataset A

# 2017

num_pos_17 = senti_17_one['preprocess_txt'].map(lambda x: len([i for i in x if i in pos_words]))
senti_17_one['pos_count'] = num_pos_17
num_neg_17 = senti_17_one['preprocess_txt'].map(lambda x: len([i for i in x if i in neg_words]))
senti_17_one['neg_count'] = num_neg_17

# 2018

num_pos_18 = senti_18_one['preprocess_txt'].map(lambda x: len([i for i in x if i in pos_words]))
senti_18_one['pos_count'] = num_pos_18
num_neg_18 = senti_18_one['preprocess_txt'].map(lambda x: len([i for i in x if i in neg_words]))
senti_18_one['neg_count'] = num_neg_18

# 2019

num_pos_19 = senti_19_one['preprocess_txt'].map(lambda x: len([i for i in x if i in pos_words]))
senti_19_one['pos_count'] = num_pos_19
num_neg_19 = senti_19_one['preprocess_txt'].map(lambda x: len([i for i in x if i in neg_words]))
senti_19_one['neg_count'] = num_neg_19

senti_17_one['sentiment'] = round(senti_17_one['pos_count'] / (senti_17_one['neg_count']+1), 2)
senti_18_one['sentiment'] = round(senti_18_one['pos_count'] / (senti_18_one['neg_count']+1), 2)
senti_19_one['sentiment'] = round(senti_19_one['pos_count'] / (senti_19_one['neg_count']+1), 2)

# dataset B

words_2 = pd.read_excel(f'{data_path_text}Positive and Negative Word List.xlsx',index_col=0) # https://www.kaggle.com/datasets/mukulkirti/positive-and-negative-word-listrar
pos_words_2 = words_2['Positive Sense Word List'].tolist()
neg_words_2 = words_2['Negative Sense Word List'].tolist()

# calculate sentiment scores from dataset B

# 2017

num_pos_17_2 = senti_17_one['preprocess_txt'].map(lambda x: len([i for i in x if i in pos_words_2]))
senti_17_one['pos_count_2'] = num_pos_17_2
num_neg_17_2 = senti_17_one['preprocess_txt'].map(lambda x: len([i for i in x if i in neg_words_2]))
senti_17_one['neg_count_2'] = num_neg_17_2

# 2018

num_pos_18_2 = senti_18_one['preprocess_txt'].map(lambda x: len([i for i in x if i in pos_words_2]))
senti_18_one['pos_count_2'] = num_pos_18_2
num_neg_18_2 = senti_18_one['preprocess_txt'].map(lambda x: len([i for i in x if i in neg_words_2]))
senti_18_one['neg_count_2'] = num_neg_18_2

# 2019

num_pos_19_2 = senti_19_one['preprocess_txt'].map(lambda x: len([i for i in x if i in pos_words_2]))
senti_19_one['pos_count_2'] = num_pos_19_2
num_neg_19_2 = senti_19_one['preprocess_txt'].map(lambda x: len([i for i in x if i in neg_words_2]))
senti_19_one['neg_count_2'] = num_neg_19_2

senti_17_one['sentiment_2'] = round(senti_17_one['pos_count_2'] / (senti_17_one['neg_count_2']+1), 2)
senti_18_one['sentiment_2'] = round(senti_18_one['pos_count_2'] / (senti_18_one['neg_count_2']+1), 2)
senti_19_one['sentiment_2'] = round(senti_19_one['pos_count_2'] / (senti_19_one['neg_count_2']+1), 2)

"""**Sentiment Score by Vader-analyzer**"""

def pos_neg_words_new(rev):
  pos_words_list = []
  neg_words_list = []
  tokens = rev.split()
  for i in range(len(tokens)):
    score = vader_sentiment.polarity_scores(tokens[i])
    if score['compound']>.05:
      if tokens[i-1] == 'not':
        neg_words_list.append(f'{tokens[i-1]}-{tokens[i]}')
      else:
        pos_words_list.append(tokens[i])
    elif score['compound']<-.01:
      neg_words_list.append(tokens[i])
  pos_words_list = list(set(pos_words_list))
  neg_words_list = list(set(neg_words_list))
  return pos_words_list,neg_words_list

def pos_neg_words_vader(rev):
  pos_words_list = []
  neg_words_list = []
  tokens = rev.split()
  for i in range(len(tokens)):
    score = vader_sentiment.polarity_scores(tokens[i])
    if score['compound']>.05:
      if tokens[i-1] not in ['not','would','could','might']:
        neg_words_list.append(f'{tokens[i-1]}-{tokens[i]}')
      else:
        pos_words_list.append(tokens[i])
    elif score['compound']<-.05:
      neg_words_list.append(tokens[i])
  pos_words_list = list(set(pos_words_list))
  neg_words_list = list(set(neg_words_list))
  return pos_words_list,neg_words_list

senti_17_one['processed_text'] = senti_17_one['rev_new'].apply(lambda x:processing(x))
senti_18_one['processed_text'] = senti_18_one['rev_new'].apply(lambda x:processing(x))
senti_19_one['processed_text'] = senti_19_one['rev_new'].apply(lambda x:processing(x))

senti_17_one['pos_words_v'] = senti_17_one.processed_text.map(lambda x:pos_neg_words_new(x)[0])#get_common_words
senti_17_one['neg_words_v'] = senti_17_one.processed_text.apply(lambda x:pos_neg_words_new(x)[1])
senti_17_one['pos_count_v'] = senti_17_one.pos_words_v.apply(lambda x:len(x))
senti_17_one['neg_count_v'] = senti_17_one.neg_words_v.apply(lambda x:len(x))
senti_17_one['sentiment_v'] = round(senti_17_one['pos_count_v'] / (senti_17_one['neg_count_v']+1), 2)

senti_18_one['pos_words_v'] = senti_18_one.processed_text.map(lambda x:pos_neg_words_new(x)[0])#get_common_words
senti_18_one['neg_words_v'] = senti_18_one.processed_text.apply(lambda x:pos_neg_words_new(x)[1])
senti_18_one['pos_count_v'] = senti_18_one.pos_words_v.apply(lambda x:len(x))
senti_18_one['neg_count_v'] = senti_18_one.neg_words_v.apply(lambda x:len(x))
senti_18_one['sentiment_v'] = round(senti_18_one['pos_count_v'] / (senti_18_one['neg_count_v']+1), 2)

senti_19_one['pos_words_v'] = senti_19_one.processed_text.map(lambda x:pos_neg_words_new(x)[0])#get_common_words
senti_19_one['neg_words_v'] = senti_19_one.processed_text.apply(lambda x:pos_neg_words_new(x)[1])
senti_19_one['pos_count_v'] = senti_19_one.pos_words_v.apply(lambda x:len(x))
senti_19_one['neg_count_v'] = senti_19_one.neg_words_v.apply(lambda x:len(x))
senti_19_one['sentiment_v'] = round(senti_19_one['pos_count_v'] / (senti_19_one['neg_count_v']+1), 2)

fig, ax = plt.subplots(3, 3, figsize=(16, 13.5),sharey=True)
order = ['Senior','Intermediate','Junior']
sns.barplot(senti_17_one,y='sentiment',x='auth_category',ax=ax[0,0],order=order,errorbar=None)
sns.barplot(senti_18_one,y='sentiment',x='auth_category', ax=ax[0,1],order=order,errorbar=None)
sns.barplot(senti_19_one,y='sentiment',x='auth_category', ax=ax[0,2],order=order,errorbar=None)

sns.barplot(senti_17_one,y='sentiment_2',x='auth_category',ax=ax[1,0],order=order,errorbar=None)
sns.barplot(senti_18_one,y='sentiment_2',x='auth_category', ax=ax[1,1],order=order,errorbar=None)
sns.barplot(senti_19_one,y='sentiment_2',x='auth_category', ax=ax[1,2],order=order,errorbar=None)

sns.barplot(senti_17_one,y='sentiment_v',x='auth_category',ax=ax[2,0],order=order,errorbar=None)
sns.barplot(senti_18_one,y='sentiment_v',x='auth_category', ax=ax[2,1],order=order,errorbar=None)
sns.barplot(senti_19_one,y='sentiment_v',x='auth_category', ax=ax[2,2],order=order,errorbar=None)

ax[0,0].bar_label(ax[0,0].containers[0], fmt='%.2f',weight='bold',fontsize=13)
ax[0,1].bar_label(ax[0,1].containers[0], fmt='%.2f',weight='bold',fontsize=13)
ax[0,2].bar_label(ax[0,2].containers[0], fmt='%.2f',weight='bold',fontsize=13)
ax[1,0].bar_label(ax[1,0].containers[0], fmt='%.2f',weight='bold',fontsize=13)
ax[1,1].bar_label(ax[1,1].containers[0], fmt='%.2f',weight='bold',fontsize=13)
ax[1,2].bar_label(ax[1,2].containers[0], fmt='%.2f',weight='bold',fontsize=13)
ax[2,0].bar_label(ax[2,0].containers[0], fmt='%.2f',weight='bold',fontsize=13)
ax[2,1].bar_label(ax[2,1].containers[0], fmt='%.2f',weight='bold',fontsize=13)
ax[2,2].bar_label(ax[2,2].containers[0], fmt='%.2f',weight='bold',fontsize=13)
fig.suptitle('Sentiment Score- Single Author',y=.95,fontsize=13)
ax[0,0].set_title('SB-2017')
ax[0,1].set_title('DB-2018')
ax[0,2].set_title('DB-2019')
ax[0,0].set_xlabel('')
ax[0,1].set_xlabel('')
ax[0,2].set_xlabel('')
ax[1,0].set_xlabel('')
ax[1,1].set_xlabel('')
ax[1,2].set_xlabel('')
ax[2,0].set_xlabel('')
ax[2,2].set_xlabel('')
ax[2,1].set_xlabel('\nAuthor Category',fontdict={ 'fontsize': 13})

ax[0,2].set_ylabel('')
ax[0,1].set_ylabel('')
ax[1,2].set_ylabel('')
ax[1,1].set_ylabel('')
ax[2,1].set_ylabel('')
ax[2,2].set_ylabel('')

ax[0,0].set_xticklabels(ax[0,0].get_xticklabels(), fontsize=12)
ax[0,1].set_xticklabels(ax[0,1].get_xticklabels(), fontsize=12)
ax[0,2].set_xticklabels(ax[0,2].get_xticklabels(), fontsize=12)
ax[1,0].set_xticklabels(ax[1,0].get_xticklabels(), fontsize=12)
ax[1,1].set_xticklabels(ax[1,1].get_xticklabels(), fontsize=12)
ax[1,2].set_xticklabels(ax[1,2].get_xticklabels(), fontsize=12)
ax[2,0].set_xticklabels(ax[2,0].get_xticklabels(), fontsize=12)
ax[2,1].set_xticklabels(ax[2,1].get_xticklabels(), fontsize=12)
ax[2,2].set_xticklabels(ax[2,2].get_xticklabels(), fontsize=12)

ax[0,0].set_ylabel('Score-Dataset A',fontdict={ 'fontsize': 14})
ax[1,0].set_ylabel('Score-Dataset B',fontdict={ 'fontsize': 14})
ax[2,0].set_ylabel('Score-VADER',fontdict={ 'fontsize': 14})
plt.savefig("SS_one_auth.pdf", bbox_inches="tight")
plt.show()

"""**Sentiment score from BERT**"""

!pip install transformers
from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification
from scipy.special import softmax

MODEL = f"cardiffnlp/twitter-roberta-base-sentiment"
tokenizer = AutoTokenizer.from_pretrained(MODEL)
model = AutoModelForSequenceClassification.from_pretrained(MODEL)

def roberta_polarity_scores(sentence):
    encoded_text = tokenizer(sentence, return_tensors='pt',max_length=512)
    output = model(**encoded_text)
    scores = output[0][0].detach().numpy()
    scores = softmax(scores)
    scores_dict = {
        'roberta_neg' : scores[0],
        'roberta_neu' : scores[1],
        'roberta_pos' : scores[2]
    }
    pos_score = scores_dict['roberta_pos']
    neu_score = scores_dict['roberta_neu']
    neg_score = scores_dict['roberta_neg']
    return pos_score,neu_score,neg_score

senti_17_one_rob = senti_17_one[['title','decision','auth_category','rev_new','processed_text']]
senti_18_one_rob = senti_18_one[['title','decision','auth_category','rev_new','processed_text']]
senti_19_one_rob = senti_19_one[['title','decision','auth_category','rev_new','processed_text']]

# To save time use the csv files instead of running it

# senti_17_one_rob['rob_pos'] = senti_17_one_rob.rev_new.apply(lambda x:roberta_polarity_scores(x)[0])
# senti_17_one_rob['rob_neu'] = senti_17_one_rob.rev_new.apply(lambda x:roberta_polarity_scores(x)[1])
# senti_17_one_rob['rob_neg'] = senti_17_one_rob.rev_new.apply(lambda x:roberta_polarity_scores(x)[2])

# senti_18_one_rob['rob_pos'] = senti_18_one_rob.rev_new.apply(lambda x:roberta_polarity_scores(x)[0])
# senti_18_one_rob['rob_neu'] = senti_18_one_rob.rev_new.apply(lambda x:roberta_polarity_scores(x)[1])
# senti_18_one_rob['rob_neg'] = senti_18_one_rob.rev_new.apply(lambda x:roberta_polarity_scores(x)[2])


# senti_19_one_rob['rob_pos'] = senti_19_one_rob.rev_new.apply(lambda x:roberta_polarity_scores(x)[0])
# senti_19_one_rob['rob_neu'] = senti_19_one_rob.rev_new.apply(lambda x:roberta_polarity_scores(x)[1])
# senti_19_one_rob['rob_neg'] = senti_19_one_rob.rev_new.apply(lambda x:roberta_polarity_scores(x)[2])

# senti_17_one_rob.to_csv('senti_17_rob_one.csv')
# senti_18_one_rob.to_csv('senti_18_rob_one.csv')
# senti_19_one_rob.to_csv('senti_19_rob_one.csv')

senti_17_one_rob = pd.read_csv(f'{data_path_text}senti_17_rob_one.csv',index_col=0)
senti_18_one_rob = pd.read_csv(f'{data_path_text}senti_18_rob_one.csv',index_col=0)
senti_19_one_rob = pd.read_csv(f'{data_path_text}senti_19_rob_one.csv',index_col=0)

senti_17_one_rob['paper_dec'] = senti_17_one_rob.decision.apply(lambda x:1 if x=='Accept' else 0)

pos = senti_17_one_rob['rob_pos'].tolist()
neg = senti_17_one_rob['rob_neg'].tolist()
y = senti_17_one_rob['paper_dec'].tolist()



fig, ax = plt.subplots(2,3, figsize=(12.5, 7.5),sharey=True)

order = ['rob_pos','rob_neu','rob_neg']
custom_palette = ["#003366","#6699ff",  "#ff6666"]

sns.barplot(senti_17_one_rob[senti_17_one_rob.decision=='Accept'],ax=ax[0,0],errorbar=None,order=order,palette=custom_palette)
sns.barplot(senti_18_one_rob[senti_18_one_rob.decision=='Accept'],ax=ax[0,1],errorbar=None,order=order,palette=custom_palette)
sns.barplot(senti_19_one_rob[senti_19_one_rob.decision=='Accept'],ax=ax[0,2],errorbar=None,order=order,palette=custom_palette)

sns.barplot(senti_17_one_rob[senti_17_one_rob.decision=='Reject'],ax=ax[1,0],errorbar=None,order=order,palette=custom_palette)
sns.barplot(senti_18_one_rob[senti_18_one_rob.decision=='Reject'],ax=ax[1,1],errorbar=None,order=order,palette=custom_palette)
sns.barplot(senti_19_one_rob[senti_19_one_rob.decision=='Reject'],ax=ax[1,2],errorbar=None,order=order,palette=custom_palette)

for container in ax[0,0].containers:
    ax[0,0].bar_label(container, fmt='%.2f',weight='bold')
for container in ax[0,1].containers:
    ax[0,1].bar_label(container, fmt='%.2f',weight='bold')
for container in ax[0,2].containers:
    ax[0,2].bar_label(container, fmt='%.2f',weight='bold')
for container in ax[1,0].containers:
    ax[1,0].bar_label(container, fmt='%.2f',weight='bold')
for container in ax[1,1].containers:
    ax[1,1].bar_label(container, fmt='%.2f',weight='bold')
for container in ax[1,2].containers:
    ax[1,2].bar_label(container, fmt='%.2f',weight='bold')
fig.suptitle('Average Sentiment Score for Accepted and Rejected Papers',y=1, fontsize=13)


ax[0,0].set_title('SB-2017')
ax[0,1].set_title('DB-2018')
ax[0,2].set_title('DB-2019')
ax[0,0].set_ylabel('Acceted papers Score',fontdict={ 'fontsize': 12})
ax[1,0].set_ylabel('Rejected papers Score',fontdict={ 'fontsize': 12})
ax[0,0].set_ylim([0,1])
ax[0,1].set_ylim([0,1])
ax[0,2].set_ylim([0,1])
plt.savefig('paper decision score-roberta.pdf')
plt.show()

senti_17_one_rob.columns

order = ['Senior','Intermediate','Junior']
# hue_order = ['Accept','Reject']
fig, ax = plt.subplots(2, 3, figsize=(14, 9.5),sharey=True)

sns.barplot(senti_17_one_rob,x='auth_category',y='rob_pos',ax=ax[0,0],errorbar =None,order=order)
sns.barplot(senti_18_one_rob,x='auth_category',y='rob_pos',ax=ax[0,1],errorbar =None,order=order)
sns.barplot(senti_19_one_rob,x='auth_category',y='rob_pos',ax=ax[0,2],errorbar =None,order=order)

# sns.barplot(senti_17_one_rob,x='auth_category',y='rob_neu',ax=ax[1,0],errorbar =None,order=order,hue_order=hue_order)
# sns.barplot(senti_18_one_rob,x='auth_category',y='rob_neu',ax=ax[1,1],errorbar =None,order=order,hue_order=hue_order)
# sns.barplot(senti_19_one_rob,x='auth_category',y='rob_neu',ax=ax[1,2],errorbar =None,order=order,hue_order=hue_order)

sns.barplot(senti_17_one_rob,x='auth_category',y='rob_neg',ax=ax[1,0],errorbar =None,order=order)
sns.barplot(senti_18_one_rob,x='auth_category',y='rob_neg',ax=ax[1,1],errorbar =None,order=order,)
sns.barplot(senti_19_one_rob,x='auth_category',y='rob_neg',ax=ax[1,2],errorbar =None,order=order)

fig.suptitle('RoBERTa Scores - Single Author',y=.98)

for container in ax[0,0].containers:
    ax[0,0].bar_label(container, fmt='%.2f',weight='bold',fontsize=13)
for container in ax[0,1].containers:
    ax[0,1].bar_label(container, fmt='%.2f',weight='bold',fontsize=13)
for container in ax[0,2].containers:
    ax[0,2].bar_label(container, fmt='%.2f',weight='bold',fontsize=13)

for container in ax[1,0].containers:
    ax[1,0].bar_label(container, fmt='%.2f',weight='bold',fontsize=13)
for container in ax[1,1].containers:
    ax[1,1].bar_label(container, fmt='%.2f',weight='bold',fontsize=13)
for container in ax[1,2].containers:
    ax[1,2].bar_label(container, fmt='%.2f',weight='bold',fontsize=13)


ax[0,0].set_title('SB-2017')
ax[0,1].set_title('DB-2018')
ax[0,2].set_title('DB-2019')

ax[0,0].set_ylabel('POS Score',fontdict={ 'fontsize': 13})
ax[0,1].set_ylabel('')
ax[0,2].set_ylabel('')
ax[1,0].set_ylabel('NEG Score',fontdict={ 'fontsize': 13})
ax[1,1].set_ylabel('')
ax[1,2].set_ylabel('')

ax[0,0].set_xlabel('')
ax[0,1].set_xlabel('')
ax[0,2].set_xlabel('')
ax[1,0].set_xlabel('')
ax[1,1].set_xlabel('\nAuthor Category',fontdict={ 'fontsize': 13})
ax[1,2].set_xlabel('')

plt.savefig("SS_one_auth_roberta.png", bbox_inches="tight")
# plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
plt.show()

""" **Sentiment analysis for paper_multi focusing on first-author
groups**
"""

rev_17['auth_len'] =  rev_17['Authors'].apply(lambda x:len(x))
rev_17['age_len'] =  rev_17['academic_age'].apply(lambda x:len(x))
rev_17['len_diff_age'] = rev_17['auth_len'] - rev_17['age_len']
rev_17 = rev_17[rev_17.len_diff_age == 0][['title','decision','review','Authors','academic_age']].reset_index(drop=True)

rev_18['auth_len'] =  rev_18['Authors'].apply(lambda x:len(x))
rev_18['age_len'] =  rev_18['academic_age'].apply(lambda x:len(x))
rev_18['len_diff_age'] = rev_18['auth_len'] - rev_18['age_len']
rev_18 = rev_18[rev_18.len_diff_age == 0][['title','decision','review','Authors','academic_age']].reset_index(drop=True)


rev_19['auth_len'] =  rev_19['Authors'].apply(lambda x:len(x))
rev_19['age_len'] =  rev_19['academic_age'].apply(lambda x:len(x))
rev_19['len_diff_age'] = rev_19['auth_len'] - rev_19['age_len']
rev_19 = rev_19[rev_19.len_diff_age == 0][['title','decision','review','Authors','academic_age']].reset_index(drop=True)

def get_age_labels(age_list):
  cat_list = []
  for age in age_list:
    if isinstance(age, str):
      cat_list.append(age)
    else:
      if age<=3:
        cat_list.append('Junior')
      elif age>3 and age<=10: # keep
        cat_list.append('Intermediate')
      elif age>10:
        cat_list.append('Senior')
  return cat_list


def get_first_auth_info(data_list):
  fa_info = [i for i in data_list][0]
  return fa_info

rev_17['author_category'] = rev_17.apply(lambda x: get_age_labels(x['academic_age']),axis=1)
rev_17['FA_category'] = rev_17.apply(lambda x: get_first_auth_info(x['author_category']),axis=1)


rev_18['author_category'] = rev_18.apply(lambda x: get_age_labels(x['academic_age']),axis=1)
rev_18['FA_category'] = rev_18.apply(lambda x: get_first_auth_info(x['author_category']),axis=1)


rev_19['author_category'] = rev_19.apply(lambda x: get_age_labels(x['academic_age']),axis=1)
rev_19['FA_category'] = rev_19.apply(lambda x: get_first_auth_info(x['author_category']),axis=1)

rev_17_fa = rev_17[['title','decision','FA_category','review']].copy()
rev_18_fa = rev_18[['title','decision','FA_category','review']].copy()
rev_19_fa = rev_19[['title','decision','FA_category','review']].copy()

def get_review(rev_list):   # combine multiple reviews per title
  rev = ' '.join(rev_list)
  return rev


rev_17_fa['rev_new'] = rev_17_fa.apply(lambda x: get_review(x['review']),axis=1)

rev_18_fa['rev_new'] = rev_18_fa.apply(lambda x: get_review(x['review']),axis=1)

rev_19_fa['rev_new'] = rev_19_fa.apply(lambda x: get_review(x['review']),axis=1)

np.random.seed(400)
rev_18_fa = rev_18_fa.sample(400)
rev_19_fa = rev_19_fa.sample(400)


rev_17_fa["preprocess_txt"] = rev_17_fa['rev_new'].apply(lambda x:text_prep(x))
rev_18_fa["preprocess_txt"] = rev_18_fa['rev_new'].apply(lambda x:text_prep(x))
rev_19_fa["preprocess_txt"] = rev_19_fa['rev_new'].apply(lambda x:text_prep(x))

## Dataset A-----------------

# 2017

num_pos_17_fa = rev_17_fa['preprocess_txt'].map(lambda x: len([i for i in x if i in pos_words]))
rev_17_fa['pos_count'] = num_pos_17_fa
num_neg_17_fa = rev_17_fa['preprocess_txt'].map(lambda x: len([i for i in x if i in neg_words]))
rev_17_fa['neg_count'] = num_neg_17_fa

# 2018

num_pos_18_fa = rev_18_fa['preprocess_txt'].map(lambda x: len([i for i in x if i in pos_words]))
rev_18_fa['pos_count'] = num_pos_18_fa
num_neg_18_fa = rev_18_fa['preprocess_txt'].map(lambda x: len([i for i in x if i in neg_words]))
rev_18_fa['neg_count'] = num_neg_18_fa

# 2019

num_pos_19_fa = rev_19_fa['preprocess_txt'].map(lambda x: len([i for i in x if i in pos_words]))
rev_19_fa['pos_count'] = num_pos_19_fa
num_neg_19_fa = rev_19_fa['preprocess_txt'].map(lambda x: len([i for i in x if i in neg_words]))
rev_19_fa['neg_count'] = num_neg_19_fa

rev_17_fa['sentiment'] = round(rev_17_fa['pos_count'] / (rev_17_fa['neg_count']+1), 2)
rev_18_fa['sentiment'] = round(rev_18_fa['pos_count'] / (rev_18_fa['neg_count']+1), 2)
rev_19_fa['sentiment'] = round(rev_19_fa['pos_count'] / (rev_19_fa['neg_count']+1), 2)

## Dataset B-----------------

# 2017

num_pos_17_fa_2 = rev_17_fa['preprocess_txt'].map(lambda x: len([i for i in x if i in pos_words_2]))
rev_17_fa['pos_count_2'] = num_pos_17_fa_2
num_neg_17_fa_2 = rev_17_fa['preprocess_txt'].map(lambda x: len([i for i in x if i in neg_words_2]))
rev_17_fa['neg_count_2'] = num_neg_17_fa_2

# 2018

num_pos_18_fa_2 = rev_18_fa['preprocess_txt'].map(lambda x: len([i for i in x if i in pos_words_2]))
rev_18_fa['pos_count_2'] = num_pos_18_fa_2
num_neg_18_fa_2 = rev_18_fa['preprocess_txt'].map(lambda x: len([i for i in x if i in neg_words_2]))
rev_18_fa['neg_count_2'] = num_neg_18_fa_2

# 2019

num_pos_19_fa_2 = rev_19_fa['preprocess_txt'].map(lambda x: len([i for i in x if i in pos_words_2]))
rev_19_fa['pos_count_2'] = num_pos_19_fa_2
num_neg_19_fa_2 = rev_19_fa['preprocess_txt'].map(lambda x: len([i for i in x if i in neg_words_2]))
rev_19_fa['neg_count_2'] = num_neg_19_fa_2

rev_17_fa['sentiment_2'] = round(rev_17_fa['pos_count_2'] / (rev_17_fa['neg_count_2']+1), 2)
rev_18_fa['sentiment_2'] = round(rev_18_fa['pos_count_2'] / (rev_18_fa['neg_count_2']+1), 2)
rev_19_fa['sentiment_2'] = round(rev_19_fa['pos_count_2'] / (rev_19_fa['neg_count_2']+1), 2)

##  Vader scores ---------------

rev_17_fa['processed_text'] = rev_17_fa['rev_new'].apply(lambda x:processing(x))
rev_18_fa['processed_text'] = rev_18_fa['rev_new'].apply(lambda x:processing(x))
rev_19_fa['processed_text'] = rev_19_fa['rev_new'].apply(lambda x:processing(x))

rev_17_fa['pos_words_v'] = rev_17_fa.processed_text.map(lambda x:pos_neg_words_vader(x)[0])#get_common_words
rev_17_fa['neg_words_v'] = rev_17_fa.processed_text.apply(lambda x:pos_neg_words_vader(x)[1])
rev_17_fa['pos_count_v'] = rev_17_fa.pos_words_v.apply(lambda x:len(x))
rev_17_fa['neg_count_v'] = rev_17_fa.neg_words_v.apply(lambda x:len(x))
rev_17_fa['sentiment_v'] = round(rev_17_fa['pos_count_v'] / (rev_17_fa['neg_count_v']+1)*100, 2)

rev_18_fa['pos_words_v'] = rev_18_fa.processed_text.map(lambda x:pos_neg_words_vader(x)[0])#get_common_words
rev_18_fa['neg_words_v'] = rev_18_fa.processed_text.apply(lambda x:pos_neg_words_vader(x)[1])
rev_18_fa['pos_count_v'] = rev_18_fa.pos_words_v.apply(lambda x:len(x))
rev_18_fa['neg_count_v'] = rev_18_fa.neg_words_v.apply(lambda x:len(x))
rev_18_fa['sentiment_v'] = round(rev_18_fa['pos_count_v'] / (rev_18_fa['neg_count_v']+1)*100, 2)

rev_19_fa['pos_words_v'] = rev_19_fa.processed_text.map(lambda x:pos_neg_words_vader(x)[0])#get_common_words
rev_19_fa['neg_words_v'] = rev_19_fa.processed_text.apply(lambda x:pos_neg_words_vader(x)[1])
rev_19_fa['pos_count_v'] = rev_19_fa.pos_words_v.apply(lambda x:len(x))
rev_19_fa['neg_count_v'] = rev_19_fa.neg_words_v.apply(lambda x:len(x))
rev_19_fa['sentiment_v'] = round(rev_19_fa['pos_count_v'] / (rev_19_fa['neg_count_v']+1)*100, 2)

fig, ax = plt.subplots(3, 3, figsize=(15, 13.5),sharey=True)
order = ['Senior','Intermediate','Junior']
sns.barplot(rev_17_fa,y='sentiment',x='FA_category',ax=ax[0,0],order=order,errorbar=None)
sns.barplot(rev_18_fa,y='sentiment',x='FA_category', ax=ax[0,1],order=order,errorbar=None)
sns.barplot(rev_19_fa,y='sentiment',x='FA_category', ax=ax[0,2],order=order,errorbar=None)

sns.barplot(rev_17_fa,y='sentiment_2',x='FA_category',ax=ax[1,0],order=order,errorbar=None)
sns.barplot(rev_18_fa,y='sentiment_2',x='FA_category', ax=ax[1,1],order=order,errorbar=None)
sns.barplot(rev_19_fa,y='sentiment_2',x='FA_category', ax=ax[1,2],order=order,errorbar=None)

sns.barplot(rev_17_fa,y='sentiment_v',x='FA_category',ax=ax[2,0],order=order,errorbar=None)
sns.barplot(rev_18_fa,y='sentiment_v',x='FA_category', ax=ax[2,1],order=order,errorbar=None)
sns.barplot(rev_19_fa,y='sentiment_v',x='FA_category', ax=ax[2,2],order=order,errorbar=None)

ax[0,0].bar_label(ax[0,0].containers[0], fmt='%.2f',weight='bold',fontsize=13)
ax[0,1].bar_label(ax[0,1].containers[0], fmt='%.2f',weight='bold',fontsize=13)
ax[0,2].bar_label(ax[0,2].containers[0], fmt='%.2f',weight='bold',fontsize=13)
ax[1,0].bar_label(ax[1,0].containers[0], fmt='%.2f',weight='bold',fontsize=13)
ax[1,1].bar_label(ax[1,1].containers[0], fmt='%.2f',weight='bold',fontsize=13)
ax[1,2].bar_label(ax[1,2].containers[0], fmt='%.2f',weight='bold',fontsize=13)
ax[2,0].bar_label(ax[2,0].containers[0], fmt='%.2f',weight='bold',fontsize=13)
ax[2,1].bar_label(ax[2,1].containers[0], fmt='%.2f',weight='bold',fontsize=13)
ax[2,2].bar_label(ax[2,2].containers[0], fmt='%.2f',weight='bold',fontsize=13)
fig.suptitle('Sentiment Score for First Authors',y=.95,)
ax[0,0].set_title('SB-2017')
ax[0,1].set_title('DB-2018')
ax[0,2].set_title('DB-2019')
ax[0,0].set_xlabel('')
ax[0,1].set_xlabel('')
ax[0,2].set_xlabel('')
ax[1,0].set_xlabel('')
ax[1,1].set_xlabel('')
ax[1,2].set_xlabel('')
ax[2,0].set_xlabel('')
ax[2,2].set_xlabel('')
ax[2,1].set_xlabel('\nFirst Author Category',fontdict={ 'fontsize': 13})

ax[0,0].set_xticklabels(ax[0,0].get_xticklabels(), fontsize=12)
ax[0,1].set_xticklabels(ax[0,1].get_xticklabels(), fontsize=12)
ax[0,2].set_xticklabels(ax[0,2].get_xticklabels(), fontsize=12)
ax[1,0].set_xticklabels(ax[1,0].get_xticklabels(), fontsize=12)
ax[1,1].set_xticklabels(ax[1,1].get_xticklabels(), fontsize=12)
ax[1,2].set_xticklabels(ax[1,2].get_xticklabels(), fontsize=12)
ax[2,0].set_xticklabels(ax[2,0].get_xticklabels(), fontsize=12)
ax[2,1].set_xticklabels(ax[2,1].get_xticklabels(), fontsize=12)
ax[2,2].set_xticklabels(ax[2,2].get_xticklabels(), fontsize=12)

ax[0,2].set_ylabel('')
ax[0,1].set_ylabel('')
ax[1,2].set_ylabel('')
ax[1,1].set_ylabel('')
ax[2,1].set_ylabel('')
ax[2,2].set_ylabel('')

ax[0,0].set_ylabel('Score-Dataset A',fontdict={ 'fontsize': 14})
ax[1,0].set_ylabel('Score-Dataset B',fontdict={ 'fontsize': 14})
ax[2,0].set_ylabel('Score-VADER',fontdict={ 'fontsize': 14})

# ax[0,0].set_ylim([0, 3])
# ax[0,1].set_ylim([0, 3])
# ax[0,2].set_ylim([0, 3])

# ax[1,0].set_ylim([0, 6.5])
# ax[1,1].set_ylim([0, 6.5])
# ax[1,2].set_ylim([0, 6.5])
plt.savefig("SS_FA_auth.pdf", format="pdf", bbox_inches="tight")
plt.show()

rev_17_fa_rob = rev_17_fa[['title','decision','FA_category','rev_new','processed_text']]
rev_18_fa_rob = rev_18_fa[['title','decision','FA_category','rev_new','processed_text']]
rev_19_fa_rob = rev_19_fa[['title','decision','FA_category','rev_new','processed_text']]

# use the csv files instead of running this

# rev_17_fa_rob['rob_pos'] = rev_17_fa_rob.rev_new.apply(lambda x:roberta_polarity_scores(x)[0])
# rev_17_fa_rob['rob_neu'] = rev_17_fa_rob.rev_new.apply(lambda x:roberta_polarity_scores(x)[1])
# rev_17_fa_rob['rob_neg'] = rev_17_fa_rob.rev_new.apply(lambda x:roberta_polarity_scores(x)[2])

# rev_18_fa_rob['rob_pos'] = rev_18_fa_rob.rev_new.apply(lambda x:roberta_polarity_scores(x)[0])
# rev_18_fa_rob['rob_neu'] = rev_18_fa_rob.rev_new.apply(lambda x:roberta_polarity_scores(x)[1])
# rev_18_fa_rob['rob_neg'] = rev_18_fa_rob.rev_new.apply(lambda x:roberta_polarity_scores(x)[2])


# rev_19_fa_rob['rob_pos'] = rev_19_fa_rob.rev_new.apply(lambda x:roberta_polarity_scores(x)[0])
# rev_19_fa_rob['rob_neu'] = rev_19_fa_rob.rev_new.apply(lambda x:roberta_polarity_scores(x)[1])
# rev_19_fa_rob['rob_neg'] = rev_19_fa_rob.rev_new.apply(lambda x:roberta_polarity_scores(x)[2])

# rev_17_fa_rob.to_csv('rev_17_fa_rob.csv')
# rev_18_fa_rob.to_csv('rev_18_fa_rob.csv')
# rev_19_fa_rob.to_csv('rev_19_fa_rob.csv')

rev_17_fa_rob_new = pd.read_csv(f'{data_path_text}rev_17_fa_rob.csv',index_col=0)
rev_18_fa_rob_new = pd.read_csv(f'{data_path_text}rev_18_fa_rob.csv',index_col=0)
rev_19_fa_rob_new = pd.read_csv(f'{data_path_text}rev_19_fa_rob.csv',index_col=0)

order = ['Senior','Intermediate','Junior']
hue_order = ['Accept','Reject']
fig, ax = plt.subplots(2, 3, figsize=(16, 8),sharey=True)

sns.barplot(rev_17_fa_rob_new,x='FA_category',y='rob_pos',ax=ax[0,0],errorbar =None,order=order,hue_order=hue_order)
sns.barplot(rev_18_fa_rob_new,x='FA_category',y='rob_pos',ax=ax[0,1],errorbar =None,order=order,hue_order=hue_order)
sns.barplot(rev_19_fa_rob_new,x='FA_category',y='rob_pos',ax=ax[0,2],errorbar =None,order=order,hue_order=hue_order)

# sns.barplot(rev_17_fa_rob_new,x='FA_category',y='rob_neu',ax=ax[1,0],errorbar =None,order=order,hue_order=hue_order)
# sns.barplot(rev_18_fa_rob_new,x='FA_category',y='rob_neu',ax=ax[1,1],errorbar =None,order=order,hue_order=hue_order)
# sns.barplot(rev_19_fa_rob_new,x='FA_category',y='rob_neu',ax=ax[1,2],errorbar =None,order=order,hue_order=hue_order)

sns.barplot(rev_17_fa_rob_new,x='FA_category',y='rob_neg',ax=ax[1,0],errorbar =None,order=order,hue_order=hue_order)
sns.barplot(rev_18_fa_rob_new,x='FA_category',y='rob_neg',ax=ax[1,1],errorbar =None,order=order,hue_order=hue_order)
sns.barplot(rev_19_fa_rob_new,x='FA_category',y='rob_neg',ax=ax[1,2],errorbar =None,order=order,hue_order=hue_order)



ax[0,0].bar_label(ax[0,0].containers[0], fmt='%.2f',weight='bold',fontsize=13)
ax[0,1].bar_label(ax[0,1].containers[0], fmt='%.2f',weight='bold',fontsize=13)
ax[0,2].bar_label(ax[0,2].containers[0], fmt='%.2f',weight='bold',fontsize=13)
ax[1,0].bar_label(ax[1,0].containers[0], fmt='%.2f',weight='bold',fontsize=13)
ax[1,1].bar_label(ax[1,1].containers[0], fmt='%.2f',weight='bold',fontsize=13)
ax[1,2].bar_label(ax[1,2].containers[0], fmt='%.2f',weight='bold',fontsize=13)

ax[0,0].set_xticklabels(ax[0,0].get_xticklabels(), fontsize=12)
ax[0,1].set_xticklabels(ax[0,1].get_xticklabels(), fontsize=12)
ax[0,2].set_xticklabels(ax[0,2].get_xticklabels(), fontsize=12)
ax[1,0].set_xticklabels(ax[1,0].get_xticklabels(), fontsize=12)
ax[1,1].set_xticklabels(ax[1,1].get_xticklabels(), fontsize=12)
ax[1,2].set_xticklabels(ax[1,2].get_xticklabels(), fontsize=12)


ax[0,0].set_title('SB-2017')
ax[0,1].set_title('DB-2018')
ax[0,2].set_title('DB-2019')

ax[0,0].set_ylabel('POS Score',fontdict={ 'fontsize': 13})
ax[0,1].set_ylabel('')
ax[0,2].set_ylabel('')
ax[1,0].set_ylabel('NEG Score',fontdict={ 'fontsize': 13})
ax[1,1].set_ylabel('')
ax[1,2].set_ylabel('')

ax[0,0].set_xlabel('')
ax[0,1].set_xlabel('')
ax[0,2].set_xlabel('')
ax[1,0].set_xlabel('')
ax[1,1].set_xlabel('\nAuthor Category',fontdict={ 'fontsize': 13})
ax[1,2].set_xlabel('')

ax[0,0].set_ylim([0, .4])
ax[0,1].set_ylim([0, .4])


# ax[1,0].set_ylim([0, .3])
# ax[1,1].set_ylim([0, .3])
# ax[1,2].set_ylim([0, .3])
fig.suptitle('roBERTa Score - First Author',y=.99)
plt.savefig("SS_FA_auth_rob.pdf",bbox_inches="tight")
plt.show()

""" **Sentiment analysis for paper_multi focusing on junior authors’
collaboration**
"""

senti_17_comb = rev_17[['title','decision','review','author_category']].copy()
senti_18_comb = rev_18[['title','decision','review','author_category']].copy()
senti_19_comb = rev_19[['title','decision','review','author_category']].copy()

def get_comb(cat_list):
  if cat_list[0] == 'Junior':
    if len(cat_list) == 1 : return 'NA' # only a single junior author
    elif len(cat_list) > 1:
      if len(set(cat_list)) <= 1 == True : return 'junior and junior' # juniors combining with juniors
      if 'Senior' in cat_list and 'Intermediate' not in cat_list : return 'junior and senior' # at least one senior with juniors, no intermed
      if 'Intermediate' in cat_list and 'Senior' not in cat_list : return 'junior and intermediate' # at least one intermed with juniors, no senior
      if all(x in cat_list for x in ['Senior', 'Intermediate']) == True : return 'junior,intermediate,senior' # juniors with at least one senior and intermed
  # elif cat_list[0] == 'Senior' and  'Intermediate' in cat_list and 'Junior' not in cat_list: return 'senior and Intermediate' # Seniors and interm
  # elif cat_list[0] == 'Intermediate' and  'Senior' in cat_list and 'Junior' not in cat_list: return 'Intermediate and senior' # Interm and seniors
  else: return 'NA'

senti_17_comb['Combination'] = senti_17_comb.author_category.apply(lambda x : get_comb(x))
senti_17_comb = senti_17_comb[~(senti_17_comb.Combination.isin(['NA','only junior']))].reset_index(drop=True)

senti_18_comb['Combination'] = senti_18_comb.author_category.apply(lambda x : get_comb(x))
senti_18_comb = senti_18_comb[~(senti_18_comb.Combination.isin(['NA','only junior']))].reset_index(drop=True)

senti_19_comb['Combination'] = senti_19_comb.author_category.apply(lambda x : get_comb(x))
senti_19_comb = senti_19_comb[~(senti_19_comb.Combination.isin(['NA','only junior']))].reset_index(drop=True)

senti_17_comb['rev_new'] = senti_17_comb.apply(lambda x: get_review(x['review']),axis=1)
senti_18_comb['rev_new'] = senti_18_comb.apply(lambda x: get_review(x['review']),axis=1)
senti_19_comb['rev_new'] = senti_19_comb.apply(lambda x: get_review(x['review']),axis=1)

senti_17_comb = senti_17_comb.drop(['author_category','review'],axis=1)
senti_18_comb = senti_18_comb.drop(['author_category','review'],axis=1)
senti_19_comb = senti_19_comb.drop(['author_category','review'],axis=1)

preprocess_tag = [text_prep(i) for i in senti_17_comb['rev_new']]
senti_17_comb["preprocess_txt"] = preprocess_tag

preprocess_tag = [text_prep(i) for i in senti_18_comb['rev_new']]
senti_18_comb["preprocess_txt"] = preprocess_tag

preprocess_tag = [text_prep(i) for i in senti_19_comb['rev_new']]
senti_19_comb["preprocess_txt"] = preprocess_tag

# dataset A

# 2017

num_pos_17_comb = senti_17_comb['preprocess_txt'].map(lambda x: len([i for i in x if i in pos_words]))
senti_17_comb['pos_count'] = num_pos_17_comb
num_neg_17_comb = senti_17_comb['preprocess_txt'].map(lambda x: len([i for i in x if i in neg_words]))
senti_17_comb['neg_count'] = num_neg_17_comb

# 2018

num_pos_18_comb = senti_18_comb['preprocess_txt'].map(lambda x: len([i for i in x if i in pos_words]))
senti_18_comb['pos_count'] = num_pos_18_comb
num_neg_18_comb = senti_18_comb['preprocess_txt'].map(lambda x: len([i for i in x if i in neg_words]))
senti_18_comb['neg_count'] = num_neg_18_comb

# 2019

num_pos_19_comb = senti_19_comb['preprocess_txt'].map(lambda x: len([i for i in x if i in pos_words]))
senti_19_comb['pos_count'] = num_pos_19_comb
num_neg_19_comb = senti_19_comb['preprocess_txt'].map(lambda x: len([i for i in x if i in neg_words]))
senti_19_comb['neg_count'] = num_neg_19_comb

senti_17_comb['sentiment'] = round(senti_17_comb['pos_count'] / (senti_17_comb['neg_count']+1), 2)
senti_18_comb['sentiment'] = round(senti_18_comb['pos_count'] / (senti_18_comb['neg_count']+1), 2)
senti_19_comb['sentiment'] = round(senti_19_comb['pos_count'] / (senti_19_comb['neg_count']+1), 2)

senti_17_comb = senti_17_comb[~(senti_17_comb.Combination.isnull())]
senti_18_comb = senti_18_comb[~(senti_18_comb.Combination.isnull())]
senti_19_comb = senti_19_comb[~(senti_19_comb.Combination.isnull())]

# dataset B

# 2017

num_pos_17_comb_2 = senti_17_comb['preprocess_txt'].map(lambda x: len([i for i in x if i in pos_words_2]))
senti_17_comb['pos_count_2'] = num_pos_17_comb_2
num_neg_17_comb_2 = senti_17_comb['preprocess_txt'].map(lambda x: len([i for i in x if i in neg_words_2]))
senti_17_comb['neg_count_2'] = num_neg_17_comb_2

# 2018

num_pos_18_comb_2 = senti_18_comb['preprocess_txt'].map(lambda x: len([i for i in x if i in pos_words_2]))
senti_18_comb['pos_count_2'] = num_pos_18_comb_2
num_neg_18_comb_2 = senti_18_comb['preprocess_txt'].map(lambda x: len([i for i in x if i in neg_words_2]))
senti_18_comb['neg_count_2'] = num_neg_18_comb_2

# 2019

num_pos_19_comb_2 = senti_19_comb['preprocess_txt'].map(lambda x: len([i for i in x if i in pos_words_2]))
senti_19_comb['pos_count_2'] = num_pos_19_comb_2
num_neg_19_comb_2 = senti_19_comb['preprocess_txt'].map(lambda x: len([i for i in x if i in neg_words_2]))
senti_19_comb['neg_count_2'] = num_neg_19_comb_2

senti_17_comb['sentiment_2'] = round(senti_17_comb['pos_count_2'] / (senti_17_comb['neg_count_2']+1), 2)
senti_18_comb['sentiment_2'] = round(senti_18_comb['pos_count_2'] / (senti_18_comb['neg_count_2']+1), 2)
senti_19_comb['sentiment_2'] = round(senti_19_comb['pos_count_2'] / (senti_19_comb['neg_count_2']+1), 2)

senti_17_comb['processed_text'] = senti_17_comb['rev_new'].apply(lambda x:processing(x))
senti_18_comb['processed_text'] = senti_18_comb['rev_new'].apply(lambda x:processing(x))
senti_19_comb['processed_text'] = senti_19_comb['rev_new'].apply(lambda x:processing(x))

# vader

senti_17_comb['pos_words_v'] = senti_17_comb.processed_text.map(lambda x:pos_neg_words_new(x)[0])#get_common_words
senti_17_comb['neg_words_v'] = senti_17_comb.processed_text.apply(lambda x:pos_neg_words_new(x)[1])
senti_17_comb['pos_count_v'] = senti_17_comb.pos_words_v.apply(lambda x:len(x))
senti_17_comb['neg_count_v'] = senti_17_comb.neg_words_v.apply(lambda x:len(x))
senti_17_comb['sentiment_v'] = round(senti_17_comb['pos_count_v'] / (senti_17_comb['neg_count_v']+1), 2)

senti_18_comb['pos_words_v'] = senti_18_comb.processed_text.map(lambda x:pos_neg_words_new(x)[0])#get_common_words
senti_18_comb['neg_words_v'] = senti_18_comb.processed_text.apply(lambda x:pos_neg_words_new(x)[1])
senti_18_comb['pos_count_v'] = senti_18_comb.pos_words_v.apply(lambda x:len(x))
senti_18_comb['neg_count_v'] = senti_18_comb.neg_words_v.apply(lambda x:len(x))
senti_18_comb['sentiment_v'] = round(senti_18_comb['pos_count_v'] / (senti_18_comb['neg_count_v']+1), 2)

senti_19_comb['pos_words_v'] = senti_19_comb.processed_text.map(lambda x:pos_neg_words_new(x)[0])#get_common_words
senti_19_comb['neg_words_v'] = senti_19_comb.processed_text.apply(lambda x:pos_neg_words_new(x)[1])
senti_19_comb['pos_count_v'] = senti_19_comb.pos_words_v.apply(lambda x:len(x))
senti_19_comb['neg_count_v'] = senti_19_comb.neg_words_v.apply(lambda x:len(x))
senti_19_comb['sentiment_v'] = round(senti_19_comb['pos_count_v'] / (senti_19_comb['neg_count_v']+1), 2)

fig, ax = plt.subplots(3, 3, figsize=(16, 12),sharex=True)
order = ['junior,intermediate,senior','junior and senior','junior and intermediate','junior and junior']

sns.barplot(senti_17_comb,y='Combination',x=senti_17_comb['sentiment'],ax=ax[0,0],order=order,errorbar=None,palette='rocket')
sns.barplot(senti_18_comb,y='Combination',x=senti_18_comb['sentiment'],ax=ax[0,1],order=order,errorbar=None,palette='rocket').set(yticklabels=[])
sns.barplot(senti_19_comb,y='Combination',x=senti_19_comb['sentiment'],ax=ax[0,2],order=order,errorbar=None,palette='rocket').set(yticklabels=[])

sns.barplot(senti_17_comb,y='Combination',x=senti_17_comb['sentiment_2'],ax=ax[1,0],order=order,errorbar=None,palette='rocket')
sns.barplot(senti_18_comb,y='Combination',x=senti_18_comb['sentiment_2'],ax=ax[1,1],order=order,errorbar=None,palette='rocket').set(yticklabels=[])
sns.barplot(senti_19_comb,y='Combination',x=senti_19_comb['sentiment_2'],ax=ax[1,2],order=order,errorbar=None,palette='rocket').set(yticklabels=[])


sns.barplot(senti_17_comb,y='Combination',x=senti_17_comb['sentiment_v'],ax=ax[2,0],order=order,errorbar=None,palette='rocket')
sns.barplot(senti_18_comb,y='Combination',x=senti_18_comb['sentiment_v'],ax=ax[2,1],order=order,errorbar=None,palette='rocket').set(yticklabels=[])
sns.barplot(senti_19_comb,y='Combination',x=senti_19_comb['sentiment_v'],ax=ax[2,2],order=order,errorbar=None,palette='rocket').set(yticklabels=[])

ax[0,0].bar_label(ax[0,0].containers[0], fmt='%.2f',weight='bold',fontsize=13)
ax[0,1].bar_label(ax[0,1].containers[0], fmt='%.2f',weight='bold',fontsize=13)
ax[0,2].bar_label(ax[0,2].containers[0], fmt='%.2f',weight='bold',fontsize=13)
ax[1,0].bar_label(ax[1,0].containers[0], fmt='%.2f',weight='bold',fontsize=13)
ax[1,1].bar_label(ax[1,1].containers[0], fmt='%.2f',weight='bold',fontsize=13)
ax[1,2].bar_label(ax[1,2].containers[0], fmt='%.2f',weight='bold',fontsize=13)
ax[2,0].bar_label(ax[2,0].containers[0], fmt='%.2f',weight='bold',fontsize=13)
ax[2,1].bar_label(ax[2,1].containers[0], fmt='%.2f',weight='bold',fontsize=13)
ax[2,2].bar_label(ax[2,2].containers[0], fmt='%.2f',weight='bold',fontsize=13)
fig.suptitle('Sentiment Score- Junior Author Collaboration',y=.95,fontsize=13)#palette='rocket'
ax[0,0].set_title('SB-2017')
ax[0,1].set_title('DB-2018')
ax[0,2].set_title('DB-2019')
ax[0,0].set_xlabel('')
ax[0,1].set_xlabel('')
ax[0,2].set_xlabel('')
ax[1,0].set_xlabel('')
ax[1,1].set_xlabel('')
ax[1,2].set_xlabel('')
ax[2,0].set_xlabel('')
ax[2,2].set_xlabel('')

ax[0,0].set_ylabel('')
ax[1,0].set_ylabel('')
ax[2,0].set_ylabel('')
ax[0,2].set_ylabel('')
ax[0,1].set_ylabel('')
ax[1,2].set_ylabel('')
ax[1,1].set_ylabel('')
ax[2,1].set_ylabel('')
ax[2,2].set_ylabel('')

ax[0,0].set_xlim([0, 7])
ax[0,1].set_xlim([0, 7])
ax[0,2].set_xlim([0, 7])

ax[0,0].set_yticklabels(ax[0,0].get_yticklabels(), fontsize=14)
ax[1,0].set_yticklabels(ax[1,0].get_yticklabels(), fontsize=14)
ax[2,0].set_yticklabels(ax[2,0].get_yticklabels(), fontsize=14)


ax[0,1].set_xlabel('Score-Dataset A',fontdict={ 'fontsize': 13})
ax[1,1].set_xlabel('Score-Dataset B',fontdict={ 'fontsize': 13})
ax[2,1].set_xlabel('Score-VADER',fontdict={ 'fontsize': 13})
plt.savefig("SS_comb.pdf", bbox_inches="tight")
plt.show()

senti_17_comb_rob = senti_17_comb[['title','decision','Combination','rev_new','processed_text']]
senti_18_comb_rob = senti_18_comb[['title','decision','Combination','rev_new','processed_text']]
senti_19_comb_rob = senti_19_comb[['title','decision','Combination','rev_new','processed_text']]

senti_19_comb_rob['Combination'].value_counts()

# senti_17_comb_rob['rob_pos'] = senti_17_comb_rob.rev_new.apply(lambda x:roberta_polarity_scores(x)[0])
# senti_17_comb_rob['rob_neu'] = senti_17_comb_rob.rev_new.apply(lambda x:roberta_polarity_scores(x)[1])
# senti_17_comb_rob['rob_neg'] = senti_17_comb_rob.rev_new.apply(lambda x:roberta_polarity_scores(x)[2])

# senti_18_comb_rob['rob_pos'] = senti_18_comb_rob.rev_new.apply(lambda x:roberta_polarity_scores(x)[0])
# senti_18_comb_rob['rob_neu'] = senti_18_comb_rob.rev_new.apply(lambda x:roberta_polarity_scores(x)[1])
# senti_18_comb_rob['rob_neg'] = senti_18_comb_rob.rev_new.apply(lambda x:roberta_polarity_scores(x)[2])


# senti_19_comb_rob['rob_pos'] = senti_19_comb_rob.rev_new.apply(lambda x:roberta_polarity_scores(x)[0])
# senti_19_comb_rob['rob_neu'] = senti_19_comb_rob.rev_new.apply(lambda x:roberta_polarity_scores(x)[1])
# senti_19_comb_rob['rob_neg'] = senti_19_comb_rob.rev_new.apply(lambda x:roberta_polarity_scores(x)[2])

# senti_17_comb_rob.to_csv('senti_17_rob_comb.csv')
# senti_18_comb_rob.to_csv('senti_18_rob_comb.csv')
# senti_19_comb_rob.to_csv('senti_19_rob_comb.csv')

senti_17_comb_rob = pd.read_csv(f'{data_path_text}senti_17_rob_comb (1).csv',index_col=0)
senti_18_comb_rob = pd.read_csv(f'{data_path_text}senti_18_rob_comb (1).csv',index_col=0)
senti_19_comb_rob = pd.read_csv(f'{data_path_text}senti_19_rob_comb (1).csv',index_col=0)

fig, ax = plt.subplots(2, 3, figsize=(14, 8),sharex=True)
order = ['junior,intermediate,senior','junior and senior','junior and intermediate','junior and junior']

sns.barplot(x = 'rob_pos',data = senti_17_comb_rob,y='Combination',errorbar=None,order=order,ax=ax[0,0],palette='rocket')
sns.barplot(x = 'rob_pos',data = senti_18_comb_rob,y='Combination',errorbar=None,order=order,ax=ax[0,1],palette='rocket').set(yticklabels=[])
sns.barplot(x = 'rob_pos',data = senti_19_comb_rob,y='Combination',errorbar=None,order=order,ax=ax[0,2],palette='rocket').set(yticklabels=[])

sns.barplot(x = 'rob_neg',data = senti_17_comb_rob,y='Combination',errorbar=None,order=order,ax=ax[1,0],palette='rocket')
sns.barplot(x = 'rob_neg',data = senti_18_comb_rob,y='Combination',errorbar=None,order=order,ax=ax[1,1],palette='rocket').set(yticklabels=[])
sns.barplot(x = 'rob_neg',data = senti_19_comb_rob,y='Combination',errorbar=None,order=order,ax=ax[1,2],palette='rocket').set(yticklabels=[])

for container in ax[0,0].containers:
    ax[0,0].bar_label(container, fmt='%.2f',weight='bold',fontsize=13)
for container in ax[0,1].containers:
    ax[0,1].bar_label(container, fmt='%.2f',weight='bold',fontsize=13)
for container in ax[0,2].containers:
    ax[0,2].bar_label(container, fmt='%.2f',weight='bold',fontsize=13)
for container in ax[1,0].containers:
    ax[1,0].bar_label(container, fmt='%.2f',weight='bold',fontsize=13)
for container in ax[1,1].containers:
    ax[1,1].bar_label(container, fmt='%.2f',weight='bold',fontsize=13)
for container in ax[1,2].containers:
    ax[1,2].bar_label(container, fmt='%.2f',weight='bold',fontsize=13)

fig.suptitle('roBERTa Sentiment Score- Junior Author Collaboration',y=.95,fontsize=13)
ax[0,0].set_title('SB-2017')
ax[0,1].set_title('DB-2018')
ax[0,2].set_title('DB-2019')
ax[0,0].set_xlabel('')
ax[0,1].set_xlabel('POS Score\n',fontdict={ 'fontsize': 13})
ax[0,2].set_xlabel('')
ax[1,0].set_xlabel('')
ax[1,1].set_xlabel('NEG Score',fontdict={ 'fontsize': 13})
ax[1,2].set_xlabel('')

ax[0,0].set_ylabel('')
ax[0,2].set_ylabel('')
ax[0,1].set_ylabel('')
ax[1,2].set_ylabel('')
ax[1,1].set_ylabel('')
ax[1,0].set_ylabel('')
ax[0,0].set_xlim([0, .5])
ax[0,1].set_xlim([0, .50])
ax[0,2].set_xlim([0, .50])

ax[0,0].set_yticklabels(ax[0,0].get_yticklabels(), fontsize=14)
ax[1,0].set_yticklabels(ax[1,0].get_yticklabels(), fontsize=14)

plt.savefig("SS_comb_rob.pdf", bbox_inches="tight")
plt.show()

"""# **Statistical Check:-**

https://www.kaggle.com/code/hamelg/python-for-data-26-anova
"""

import scipy.stats as stats

"""**Comparison between author combinations within a single year**"""

# 2017

comb_JS = senti_17_comb[senti_17_comb["Combination"] == 'junior and senior']['sentiment'].tolist()
comb_JJ = senti_17_comb[senti_17_comb["Combination"] == 'junior and junior']['sentiment'].tolist()
comb_JSI = senti_17_comb[senti_17_comb["Combination"] == 'junior,intermediate,senior']['sentiment'].tolist()
comb_JI = senti_17_comb[senti_17_comb["Combination"] == 'junior and intermediate']['sentiment'].tolist()
print('2017 results dataset A: ',stats.f_oneway(comb_JS, comb_JJ, comb_JSI, comb_JI))

comb_JS = senti_17_comb[senti_17_comb["Combination"] == 'junior and senior']['sentiment_2'].tolist()
comb_JJ = senti_17_comb[senti_17_comb["Combination"] == 'junior and junior']['sentiment_2'].tolist()
comb_JSI = senti_17_comb[senti_17_comb["Combination"] == 'junior,intermediate,senior']['sentiment_2'].tolist()
comb_JI = senti_17_comb[senti_17_comb["Combination"] == 'junior and intermediate']['sentiment_2'].tolist()
print('2017 results dataset B: ',stats.f_oneway(comb_JS, comb_JJ, comb_JSI, comb_JI))

comb_JS = senti_17_comb[senti_17_comb["Combination"] == 'junior and senior']['sentiment_v'].tolist()
comb_JJ = senti_17_comb[senti_17_comb["Combination"] == 'junior and junior']['sentiment_v'].tolist()
comb_JSI = senti_17_comb[senti_17_comb["Combination"] == 'junior,intermediate,senior']['sentiment_v'].tolist()
comb_JI = senti_17_comb[senti_17_comb["Combination"] == 'junior and intermediate']['sentiment_v'].tolist()
print('2017 results VADER: ',stats.f_oneway(comb_JS, comb_JJ, comb_JSI, comb_JI))

comb_JS = senti_17_comb_rob[senti_17_comb_rob["Combination"] == 'junior and senior']['rob_pos'].tolist()
comb_JJ = senti_17_comb_rob[senti_17_comb_rob["Combination"] == 'junior and junior']['rob_pos'].tolist()
comb_JSI = senti_17_comb_rob[senti_17_comb_rob["Combination"] == 'junior,intermediate,senior']['rob_pos'].tolist()
comb_JI = senti_17_comb_rob[senti_17_comb_rob["Combination"] == 'junior and intermediate']['rob_pos'].tolist()
print('2017 roBERTa positive:',stats.f_oneway(comb_JS, comb_JJ, comb_JSI, comb_JI))

comb_JS = senti_17_comb_rob[senti_17_comb_rob["Combination"] == 'junior and senior']['rob_neg'].tolist()
comb_JJ = senti_17_comb_rob[senti_17_comb_rob["Combination"] == 'junior and junior']['rob_neg'].tolist()
comb_JSI = senti_17_comb_rob[senti_17_comb_rob["Combination"] == 'junior,intermediate,senior']['rob_neg'].tolist()
comb_JI = senti_17_comb_rob[senti_17_comb_rob["Combination"] == 'junior and intermediate']['rob_neg'].tolist()
print('2017 roBERTa negative:',stats.f_oneway(comb_JS, comb_JJ, comb_JSI, comb_JI))

# 2018

comb_JS = senti_18_comb[senti_18_comb["Combination"] == 'junior and senior']['sentiment'].tolist()
comb_JJ = senti_18_comb[senti_18_comb["Combination"] == 'junior and junior']['sentiment'].tolist()
comb_JSI = senti_18_comb[senti_18_comb["Combination"] == 'junior,intermediate,senior']['sentiment'].tolist()
comb_JI = senti_18_comb[senti_18_comb["Combination"] == 'junior and intermediate']['sentiment'].tolist()
print('2018 results dataset A: ',stats.f_oneway(comb_JS, comb_JJ, comb_JSI, comb_JI))

comb_JS = senti_18_comb[senti_18_comb["Combination"] == 'junior and senior']['sentiment_2'].tolist()
comb_JJ = senti_18_comb[senti_18_comb["Combination"] == 'junior and junior']['sentiment_2'].tolist()
comb_JSI = senti_18_comb[senti_18_comb["Combination"] == 'junior,intermediate,senior']['sentiment_2'].tolist()
comb_JI = senti_18_comb[senti_18_comb["Combination"] == 'junior and intermediate']['sentiment_2'].tolist()
print('2018 results dataset B: ',stats.f_oneway(comb_JS, comb_JJ, comb_JSI, comb_JI))

comb_JS = senti_18_comb[senti_18_comb["Combination"] == 'junior and senior']['sentiment_v'].tolist()
comb_JJ = senti_18_comb[senti_18_comb["Combination"] == 'junior and junior']['sentiment_v'].tolist()
comb_JSI = senti_18_comb[senti_18_comb["Combination"] == 'junior,intermediate,senior']['sentiment_v'].tolist()
comb_JI = senti_18_comb[senti_18_comb["Combination"] == 'junior and intermediate']['sentiment_v'].tolist()
print('2018 results VADER: ',stats.f_oneway(comb_JS, comb_JJ, comb_JSI, comb_JI))

comb_JS = senti_18_comb_rob[senti_18_comb_rob["Combination"] == 'junior and senior']['rob_pos'].tolist()
comb_JJ = senti_18_comb_rob[senti_18_comb_rob["Combination"] == 'junior and junior']['rob_pos'].tolist()
comb_JSI = senti_18_comb_rob[senti_18_comb_rob["Combination"] == 'junior,intermediate,senior']['rob_pos'].tolist()
comb_JI = senti_18_comb_rob[senti_18_comb_rob["Combination"] == 'junior and intermediate']['rob_pos'].tolist()
print('2018 roBERTa positive:',stats.f_oneway(comb_JS, comb_JJ, comb_JSI, comb_JI))

comb_JS = senti_18_comb_rob[senti_18_comb_rob["Combination"] == 'junior and senior']['rob_neg'].tolist()
comb_JJ = senti_18_comb_rob[senti_18_comb_rob["Combination"] == 'junior and junior']['rob_neg'].tolist()
comb_JSI = senti_18_comb_rob[senti_18_comb_rob["Combination"] == 'junior,intermediate,senior']['rob_neg'].tolist()
comb_JI = senti_18_comb_rob[senti_18_comb_rob["Combination"] == 'junior and intermediate']['rob_neg'].tolist()
print('2018 roBERTa negative:',stats.f_oneway(comb_JS, comb_JJ, comb_JSI, comb_JI))

# 2019

comb_JS = senti_19_comb[senti_19_comb["Combination"] == 'junior and senior']['sentiment'].tolist()
comb_JJ = senti_19_comb[senti_19_comb["Combination"] == 'junior and junior']['sentiment'].tolist()
comb_JSI = senti_19_comb[senti_19_comb["Combination"] == 'junior,intermediate,senior']['sentiment'].tolist()
comb_JI = senti_19_comb[senti_19_comb["Combination"] == 'junior and intermediate']['sentiment'].tolist()
print('2019 results dataset A: ',stats.f_oneway(comb_JS, comb_JJ, comb_JSI, comb_JI))

comb_JS = senti_19_comb[senti_19_comb["Combination"] == 'junior and senior']['sentiment_2'].tolist()
comb_JJ = senti_19_comb[senti_19_comb["Combination"] == 'junior and junior']['sentiment_2'].tolist()
comb_JSI = senti_19_comb[senti_19_comb["Combination"] == 'junior,intermediate,senior']['sentiment_2'].tolist()
comb_JI = senti_19_comb[senti_19_comb["Combination"] == 'junior and intermediate']['sentiment_2'].tolist()
print('2019 results dataset B: ',stats.f_oneway(comb_JS, comb_JJ, comb_JSI, comb_JI))

comb_JS = senti_19_comb[senti_19_comb["Combination"] == 'junior and senior']['sentiment_v'].tolist()
comb_JJ = senti_19_comb[senti_19_comb["Combination"] == 'junior and junior']['sentiment_v'].tolist()
comb_JSI = senti_19_comb[senti_19_comb["Combination"] == 'junior,intermediate,senior']['sentiment_v'].tolist()
comb_JI = senti_19_comb[senti_19_comb["Combination"] == 'junior and intermediate']['sentiment_v'].tolist()
print('2019 results VADER: ',stats.f_oneway(comb_JS, comb_JJ, comb_JSI, comb_JI))

comb_JS = senti_19_comb_rob[senti_19_comb_rob["Combination"] == 'junior and senior']['rob_pos'].tolist()
comb_JJ = senti_19_comb_rob[senti_19_comb_rob["Combination"] == 'junior and junior']['rob_pos'].tolist()
comb_JSI = senti_19_comb_rob[senti_19_comb_rob["Combination"] == 'junior,intermediate,senior']['rob_pos'].tolist()
comb_JI = senti_19_comb_rob[senti_19_comb_rob["Combination"] == 'junior and intermediate']['rob_pos'].tolist()
print('2019 roBERTa positive:',stats.f_oneway(comb_JS, comb_JJ, comb_JSI, comb_JI))

comb_JS = senti_19_comb_rob[senti_19_comb_rob["Combination"] == 'junior and senior']['rob_neg'].tolist()
comb_JJ = senti_19_comb_rob[senti_19_comb_rob["Combination"] == 'junior and junior']['rob_neg'].tolist()
comb_JSI = senti_19_comb_rob[senti_19_comb_rob["Combination"] == 'junior,intermediate,senior']['rob_neg'].tolist()
comb_JI = senti_19_comb_rob[senti_19_comb_rob["Combination"] == 'junior and intermediate']['rob_neg'].tolist()
print('2019 roBERTa negative:',stats.f_oneway(comb_JS, comb_JJ, comb_JSI, comb_JI))



""" **Comparison between author categories within single-blind and
double-blind years**
"""

# junior and senior

comb_JS_17_1 = senti_17_comb[senti_17_comb["Combination"] == 'junior and senior']['sentiment'].tolist()
comb_JS_18_1 = senti_18_comb[senti_18_comb["Combination"] == 'junior and senior']['sentiment'].tolist()
comb_JS_19_1 = senti_19_comb[senti_19_comb["Combination"] == 'junior and senior']['sentiment'].tolist()

comb_JS_17_2 = senti_17_comb[senti_17_comb["Combination"] == 'junior and senior']['sentiment_2'].tolist()
comb_JS_18_2 = senti_18_comb[senti_18_comb["Combination"] == 'junior and senior']['sentiment_2'].tolist()
comb_JS_19_2 = senti_19_comb[senti_19_comb["Combination"] == 'junior and senior']['sentiment_2'].tolist()

comb_JS_17_v = senti_17_comb[senti_17_comb["Combination"] == 'junior and senior']['sentiment_v'].tolist()
comb_JS_18_v = senti_18_comb[senti_18_comb["Combination"] == 'junior and senior']['sentiment_v'].tolist()
comb_JS_19_v = senti_19_comb[senti_19_comb["Combination"] == 'junior and senior']['sentiment_v'].tolist()

print('junior and senior:::::::::::::\n')
print('dataset A: ',stats.f_oneway(comb_JS_17_1, comb_JS_18_1, comb_JS_19_1 ))
print('dataset B: ',stats.f_oneway(comb_JS_17_2, comb_JS_18_2, comb_JS_19_2 ))
print('dataset VADER: ',stats.f_oneway(comb_JS_17_v, comb_JS_18_v, comb_JS_19_v ))

# roberta

comb_JS1 = senti_17_comb_rob[senti_17_comb_rob["Combination"] == 'junior and senior']['rob_neg'].tolist()
comb_JS2 = senti_18_comb_rob[senti_18_comb_rob["Combination"] == 'junior and senior']['rob_neg'].tolist()
comb_JS3 = senti_19_comb_rob[senti_19_comb_rob["Combination"] == 'junior and senior']['rob_neg'].tolist()
print('NEG',stats.f_oneway(comb_JS1, comb_JS2, comb_JS3, ))

comb_JS1 = senti_17_comb_rob[senti_17_comb_rob["Combination"] == 'junior and senior']['rob_pos'].tolist()
comb_JS2 = senti_18_comb_rob[senti_18_comb_rob["Combination"] == 'junior and senior']['rob_pos'].tolist()
comb_JS3 = senti_19_comb_rob[senti_19_comb_rob["Combination"] == 'junior and senior']['rob_pos'].tolist()
print('POS',stats.f_oneway(comb_JS1, comb_JS2, comb_JS3, ) )

# junior and junior

comb_JJ_17_1 = senti_17_comb[senti_17_comb["Combination"] == 'junior and junior']['sentiment'].tolist()
comb_JJ_18_1 = senti_18_comb[senti_18_comb["Combination"] == 'junior and junior']['sentiment'].tolist()
comb_JJ_19_1 = senti_19_comb[senti_19_comb["Combination"] == 'junior and junior']['sentiment'].tolist()

comb_JJ_17_2 = senti_17_comb[senti_17_comb["Combination"] == 'junior and junior']['sentiment_2'].tolist()
comb_JJ_18_2 = senti_18_comb[senti_18_comb["Combination"] == 'junior and junior']['sentiment_2'].tolist()
comb_JJ_19_2 = senti_19_comb[senti_19_comb["Combination"] == 'junior and junior']['sentiment_2'].tolist()

comb_JJ_17_v = senti_17_comb[senti_17_comb["Combination"] == 'junior and junior']['sentiment_v'].tolist()
comb_JJ_18_v = senti_18_comb[senti_18_comb["Combination"] == 'junior and junior']['sentiment_v'].tolist()
comb_JJ_19_v = senti_19_comb[senti_19_comb["Combination"] == 'junior and junior']['sentiment_v'].tolist()

print('junior and junior:::::::::::::\n')
print('dataset A: ',stats.f_oneway(comb_JJ_17_1, comb_JJ_18_1, comb_JJ_19_1 ))
print('dataset B: ',stats.f_oneway(comb_JJ_17_2, comb_JJ_18_2, comb_JJ_19_2 ))
print('dataset VADER: ',stats.f_oneway(comb_JJ_17_v, comb_JJ_18_v, comb_JJ_19_v ))

# roberta

comb_JJ1 = senti_17_comb_rob[senti_17_comb_rob["Combination"] == 'junior and junior']['rob_neg'].tolist()
comb_JJ2 = senti_18_comb_rob[senti_18_comb_rob["Combination"] == 'junior and junior']['rob_neg'].tolist()
comb_JJ3 = senti_19_comb_rob[senti_19_comb_rob["Combination"] == 'junior and junior']['rob_neg'].tolist()
print('NEG',stats.f_oneway(comb_JJ1, comb_JJ2, comb_JJ3, ))

comb_JJ1 = senti_17_comb_rob[senti_17_comb_rob["Combination"] == 'junior and junior']['rob_pos'].tolist()
comb_JJ2 = senti_18_comb_rob[senti_18_comb_rob["Combination"] == 'junior and junior']['rob_pos'].tolist()
comb_JJ3 = senti_19_comb_rob[senti_19_comb_rob["Combination"] == 'junior and junior']['rob_pos'].tolist()
print('POS',stats.f_oneway(comb_JJ1, comb_JJ2, comb_JJ3, ) )

# junior Intermidate and Senior

comb_JIS_17_1 = senti_17_comb[senti_17_comb["Combination"] == 'junior,intermediate,senior']['sentiment'].tolist()
comb_JIS_18_1 = senti_18_comb[senti_18_comb["Combination"] == 'junior,intermediate,senior']['sentiment'].tolist()
comb_JIS_19_1 = senti_19_comb[senti_19_comb["Combination"] == 'junior,intermediate,senior']['sentiment'].tolist()

comb_JIS_17_2 = senti_17_comb[senti_17_comb["Combination"] == 'junior,intermediate,senior']['sentiment_2'].tolist()
comb_JIS_18_2 = senti_18_comb[senti_18_comb["Combination"] == 'junior,intermediate,senior']['sentiment_2'].tolist()
comb_JIS_19_2 = senti_19_comb[senti_19_comb["Combination"] == 'junior,intermediate,senior']['sentiment_2'].tolist()

comb_JIS_17_v = senti_17_comb[senti_17_comb["Combination"] == 'junior,intermediate,senior']['sentiment_v'].tolist()
comb_JIS_18_v = senti_18_comb[senti_18_comb["Combination"] == 'junior,intermediate,senior']['sentiment_v'].tolist()
comb_JIS_19_v = senti_19_comb[senti_19_comb["Combination"] == 'junior,intermediate,senior']['sentiment_v'].tolist()

print('junior Intermidate and Senior:::::::::::::\n')
print(stats.f_oneway(comb_JIS_17_1, comb_JIS_18_1, comb_JIS_19_1 )) # data-1 not significant
print(stats.f_oneway(comb_JIS_17_2, comb_JIS_18_2, comb_JIS_19_2 ))
print(stats.f_oneway(comb_JIS_17_v, comb_JIS_18_v, comb_JIS_19_v ))

comb_JIS1 = senti_17_comb_rob[senti_17_comb_rob["Combination"] == 'junior,intermediate,senior']['rob_neg'].tolist()
comb_JIS2 = senti_18_comb_rob[senti_18_comb_rob["Combination"] == 'junior,intermediate,senior']['rob_neg'].tolist()
comb_JIS3 = senti_19_comb_rob[senti_19_comb_rob["Combination"] == 'junior,intermediate,senior']['rob_neg'].tolist()
print('POS ',stats.f_oneway(comb_JIS1, comb_JIS2, comb_JIS3, ))

comb_JIS1 = senti_17_comb_rob[senti_17_comb_rob["Combination"] == 'junior,intermediate,senior']['rob_pos'].tolist()
comb_JIS2 = senti_18_comb_rob[senti_18_comb_rob["Combination"] == 'junior,intermediate,senior']['rob_pos'].tolist()
comb_JIS3 = senti_19_comb_rob[senti_19_comb_rob["Combination"] == 'junior,intermediate,senior']['rob_pos'].tolist()
print('NEG ',stats.f_oneway(comb_JIS1, comb_JIS2, comb_JIS3, ) )

# junior and intermediate

comb_JI_17_1 = senti_17_comb[senti_17_comb["Combination"] == 'junior and intermediate']['sentiment'].tolist()
comb_JI_18_1 = senti_18_comb[senti_18_comb["Combination"] == 'junior and intermediate']['sentiment'].tolist()
comb_JI_19_1 = senti_19_comb[senti_19_comb["Combination"] == 'junior and intermediate']['sentiment'].tolist()

comb_JI_17_2 = senti_17_comb[senti_17_comb["Combination"] == 'junior and intermediate']['sentiment_2'].tolist()
comb_JI_18_2 = senti_18_comb[senti_18_comb["Combination"] == 'junior and intermediate']['sentiment_2'].tolist()
comb_JI_19_2 = senti_19_comb[senti_19_comb["Combination"] == 'junior and intermediate']['sentiment_2'].tolist()

comb_JI_17_v = senti_17_comb[senti_17_comb["Combination"] == 'junior and intermediate']['sentiment_v'].tolist()
comb_JI_18_v = senti_18_comb[senti_18_comb["Combination"] == 'junior and intermediate']['sentiment_v'].tolist()
comb_JI_19_v = senti_19_comb[senti_19_comb["Combination"] == 'junior and intermediate']['sentiment_v'].tolist()

print('junior and intermediate:::::::::\n')
print(stats.f_oneway(comb_JI_17_1, comb_JI_18_1, comb_JI_19_1 ))
print(stats.f_oneway(comb_JI_17_2, comb_JI_18_2, comb_JI_19_2 ))
print(stats.f_oneway(comb_JI_17_v, comb_JI_18_v, comb_JI_19_v ))

comb_JI1 = senti_17_comb_rob[senti_17_comb_rob["Combination"] == 'junior and intermediate']['rob_neg'].tolist()
comb_JI2 = senti_18_comb_rob[senti_18_comb_rob["Combination"] == 'junior and intermediate']['rob_neg'].tolist()
comb_JI3 = senti_19_comb_rob[senti_19_comb_rob["Combination"] == 'junior and intermediate']['rob_neg'].tolist()
print('POS ',stats.f_oneway(comb_JI1, comb_JI2, comb_JI3, ))

comb_JI1 = senti_17_comb_rob[senti_17_comb_rob["Combination"] == 'junior and intermediate']['rob_pos'].tolist()
comb_JI2 = senti_18_comb_rob[senti_18_comb_rob["Combination"] == 'junior and intermediate']['rob_pos'].tolist()
comb_JI3 = senti_19_comb_rob[senti_19_comb_rob["Combination"] == 'junior and intermediate']['rob_pos'].tolist()
print('NEG ',stats.f_oneway(comb_JI1, comb_JI2, comb_JI3, ) )

"""**Pairwise Comparison of Junior Author Collaborations-2019**"""

# dataset A

order = ['junior,intermediate,senior','junior and senior','junior and intermediate','junior and junior']
pairs = []

for p1 in range(5):
    for p2  in range(p1+1,4):
        pairs.append((order[p1], order[p2]))

for p1, p2 in pairs:
    print(p1, p2)
    print(stats.ttest_ind(senti_19_comb[senti_19_comb["Combination"] == p1]['sentiment'].tolist(),
                          senti_19_comb[senti_19_comb["Combination"] == p2]['sentiment'].tolist()))

# dataset B

pairs = []

for p1 in range(5):
    for p2  in range(p1+1,4):
        pairs.append((order[p1], order[p2]))

for p1, p2 in pairs:
    print(p1, p2)
    print(stats.ttest_ind(senti_19_comb[senti_19_comb["Combination"] == p1]['sentiment_2'].tolist(),
                          senti_19_comb[senti_19_comb["Combination"] == p2]['sentiment_2'].tolist()))

# VADER

pairs = []

for p1 in range(5):
    for p2  in range(p1+1,4):
        pairs.append((order[p1], order[p2]))

for p1, p2 in pairs:
    print(p1, p2)
    print(stats.ttest_ind(senti_19_comb[senti_19_comb["Combination"] == p1]['sentiment_v'].tolist(),
                          senti_19_comb[senti_19_comb["Combination"] == p2]['sentiment_v'].tolist()))

# roBERTa positive

pairs = []

for p1 in range(5):
    for p2  in range(p1+1,4):
        pairs.append((order[p1], order[p2]))

for p1, p2 in pairs:
    print(p1, p2)
    print(stats.ttest_ind(senti_19_comb_rob[senti_19_comb_rob["Combination"] == p1]['rob_pos'].tolist(),
                          senti_19_comb_rob[senti_19_comb_rob["Combination"] == p2]['rob_pos'].tolist()))

# roBERTa negative

pairs = []

for p1 in range(5):
    for p2  in range(p1+1,4):
        pairs.append((order[p1], order[p2]))

for p1, p2 in pairs:
    print(p1, p2)
    print(stats.ttest_ind(senti_19_comb_rob[senti_19_comb_rob["Combination"] == p1]['rob_neg'].tolist(),
                          senti_19_comb_rob[senti_19_comb_rob["Combination"] == p2]['rob_neg'].tolist()))



"""# **Evaluation**"""

df_test = pd.read_excel(f'{data_path_text}test data.xlsx',index_col=0)

import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Load the pretrained model and tokenizer
model_name = "cardiffnlp/twitter-roberta-base-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# Define the labels corresponding to sentiment classes
labels = ["Negative", "Neutral", "Positive"]

# Function to predict sentiment labels from text
def predict_sentiment(text):
    # Tokenize the input text and prepare it for model input
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)

    # Forward pass through the model
    with torch.no_grad():
        outputs = model(**inputs)

    # Get the predicted label
    logits = outputs.logits
    predicted_class = torch.argmax(logits, dim=1).item()
    return labels[predicted_class]

df_test['predicted_label_rob'] = df_test['sentence_token'].apply(lambda x:predict_sentiment(x))
df_test['predicted_label_rob'] = df_test['predicted_label_rob'].apply(lambda x: x.lower())

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

ground_truth = df_test['Labels'].tolist()
predict_rob = df_test['predicted_label_rob'].tolist()

accuracy_rob = accuracy_score(ground_truth, predict_rob)
f1score_rob = f1_score(ground_truth, predict_rob,average='weighted')

accuracy_rob,f1score_rob